# AWS Certified Machine Learning Specialty 2025

Notes for the Udemy course: https://www.udemy.com/course/aws-machine-learning/
Additional Resources:
- Course Material: https://www.sundog-education.com/aws-certified-machine-learning-course-materials/

## Data Engineering

- Goal: to have data where it needs to be for training a ML model.

### General knowledge

- Data Partitioning: pattern for speeding up range queries, e.g. date or product
- Durability and Availability
- Data Engineering Pipelines:
    - Real-Time layer:
        Producers
            -> Kinesis Data Streams
                -> Kinesis Data Analytics
                    -> Lambda
                    -> Data Streams -> EC2 <-> SageMaker
                    -> Data Firehose
                        -> S3 -> Redshift
                        -> ElasticSearch
            -> Kinesis Data Firehose
                -> Kinesis Data Analytics
                -> S3
    - Video Layer
        Video Producers
            -> Kinesis Video Streams
                -> Rekognition
                    -> Data Streams -> (...)
                -> EC2
                    -> Data Streams -> (...)
                    <-> SageMaker
    - Batch Layer
        (
        MySQL On-premise -> DMS -> RDS -> Data Pipeline -> S3
        DynamoDB -> Data Pipeline -> S3
        ) -> S3 -> Glue ETL -> S3 <-> Batch for clean up
        * Step Functions for orchestration
        * Glue Data Catalog <- Crawlers
    - Analytics Layer
        S3
            -> EMR (Hadoop / Spark / Hive...)
            -> Redshift (Spectrum) -> QuickSight
            -> Data Catalog -> Athena -> QuickSight

### AWS Related

- AWS Budgets
- Amazon S3
    - General
        - Object storage in buckets
        - Globally unique name
        - Key is the full path
        - Partitioning
        - Max object size is 5TB
        - Object tags <key, value>
    - ML-related
        - Backbone for ML services, e.g. SageMaker
        - Data Lake
        - Centralized architecture
        - Any file format: CSV, JSON, Parquet, ORC, Avro, Protobuf
    - Bucket policies
    - Encryption
        - SSE-S3:
            - key is handled, managed, and owned by AWS S3
            - Set header: `"x-amz-server-side-encryption":"AES256"`
        - SSE-KMS:
            - keys managed by AWS KMS
            - User control + audit key usage using Cloud Trail
            - Set header: `"x-amz-server-side-encryption":"aws:kms:<arn>"`
            - Limitations on KMS service, quota per second
        - SSE-C:
            - Keys provided by customers, via HTTPS, in headers
            - Amazon S3 will not store the key
        - Client-Side Encryption
            - Clients encrypt/decrypt data outside S3
        - In transit: SSL/TLS
            - S3 exposes two endpoints HTTP & HTTPS
            - Force HTTPS using Bucket policy: `"aws:SecureTransport": "false"`
    - Force encryption using a Bucket policies, examples:
        - `"s3:x-amz-server-side-encryption": "aws:kms"`
        - `"s3:x-amz-server-size-encryption-customer-algorithm": "true"`
    - VPC Endpoint GW:
        - EC2 instances need to go through an Internet GW (public access)
        - EC2 instances need to go through an VPC Endpoint GW (private access)
            - Bucket policy: `AWS:SourceVpce` or `AWS:SourceVpc`
- AWS Kinesis
    - Streaming service alternative to Apache Kafka
    - Kinesis Streams (REAL TIME)
        - Ingesting data
        - Use cases:
            - Streams are divided into Shards / Partitions
            - Not for petabyte analysis
        - Features
            - Capacity Modes: Provisioned & On-demand
            - Producer: write 1MB/s or 1000 messages/s PER SHARD
            - Consumer: read 2MB/s or 5 API messages/s PER SHARD
            - Real-time latency: 70-200 ms
            - Data Storage for 1 to 365 days, replay capability, multi consumers
    - Kinesis Firehose (DELIVERY / INGESTION)
        - Moving (massive) data to S3 or Redshift
        - Use cases:
            - Can read from Kinesis Streams, CloudWatch or AWS IoT
            - Can use lambda to transform data
            - Batch writes into a Destination
            - Destinations:
                - AWS: S3, Redshift (via S3), ElasticSearch
                - 3rd Party: datadog, mongodb, new relic, splunk
                - Custom destinations via HTTP endpoint
            - Failed data to S3 backup bucket
        - Features:
            - Fully managed service: near real time (buffered)
            - Data Conversions CSV/JSON -> Parquet/ORC
            - Pay for data going through it
    - Kinesis Analytics
        - Real-time ETL / ML algorithms on streams
        - SQL on stream data
        - Use cases:
            - Streaming ETL: select columns, simple transformations
            - Continuous metric generation: live leader board
            - Responsive analytics: look for criteria and build alerting
        - Features
            - Pay for resources consumed (not cheap)
            - Serverless
            - IAM permissions for accessing sources and destinations
            - SQL or Flink to write the computation
            - Schema discovery
            - Lambda for preprocessing
            - Blue print
            - AWS Lambda can be also a destination
            - Managed Service for Apache Flink
                - Bring your own Flink App (Flink Sources + Flink Sinks)
        - ML integration
            - RANDOM_CUT_FOREST: anomaly detection on numeric columns based on recent history
            - HOTSPOTS: locate and return information about relatively dense regions in your data
    - Kinesis Video Streams
        - Sending video
        - Producers: AWS DeepLens, RTSP camera, etc.
            - Convention: One producer per video stream
        - Consumers:
            - Build your own model (TF, MXNet)
            - AWS SageMaker
            - Amazon Rekognition Video
        - Features:
            - Video Playback
            - Retention 1 to 10 years
        - Use cases
            - Consume stream in real-time (Video Stream)
            - Check point stream (with DynamoDB) in order to resume operation upon abort
            - Send decoded frames for ML-based inference (SageMaker)
            - Publish inference results (Data Streams)
            - Notifications (Lambda)
- Glue
    - Data Catalog
        - Metadata repository for all schemas in your account
        - Automated Schema Inferece
        - Schemas versioned
    - Crawlers
        - Iterate data to infer schemas and partitions
        - Stores: S3, Redshift, RDS
        - Partition is important to be though in advance
    - Glue ETL: Extract+Transform+Load
        - Jobs are run on a serverless Spark platform
            - Source -> Transforms -> Targets
            - Role needed (overdimensioned):
                - AmazonS3FullAccess + AWSGlueServiceRole
        - Scheduler (cron)
        - Trigger (events)
        - Transformations:
            - Bundled (DropFields, Filter, Join, Map)
            - ML (FindMatches ML): identify duplicate or matching records
            - Apache Spark transformations (e.g. K-Means)
        - Conversions between CSV, JSON, Avro, Parquet, ORC, XML
    - Glue Data Brew
        - Clean and normalize data without writing any code
        - Data source S3, Redshift, Aurora, Glue Data Catalog...
        - +250 ready-made transformation (filters, conversion, invalid values, etc.)
        - All actions are recorded into a "Recipe"
        - Then a job runs the "Recipe"
- Athena
    - Use SQL to search over multiple S3 files, that were previously crawled by Glue.
- AWS Data Stores
    - Redshift (provisioned): Data Warehousing, SQL analytics, Spectrum, OLAP (online analyitical processing)
    - RDS + Aurora (provisioned): Relational Store, OLTP (online transaction processing)
    - DynamoDB (serverless): NoSQL data store, provision R/W capacity, useful to store ML output.
    - S3: Object storage
    - OpenSearch: ElasticSearch, indexing of data
    - ElastiCache: Caching mechanism
- AWS Data Pipelines Features
    - Destinations S3, RDS, DynamoDB, Redshift
    - Manages task dependencies (orchestration)
    - Runs on EC2 instances
    - Data Pipelines vs. Glue
        - Data Pipelines give more control on underlying infrastructure (EC2, Elastic Map Reduce (EMR), etc.)
    - Data Pipelines is deprecated: https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/migration.html
        - Alternatives:
            - Glue
            - Step Functions
            - Managed Workflows for Apache Airflow (MWAA)
- AWS Batch
    - Run jobs as docker images
    - Dynamic provisioning
    - Schedule batch jobs using CloudWatch events
    - Orchestrate batch jobs using Step Functions
    - Batch vs. Glue: Batch should be for non-ETL jobs.
- AWS DMS:
    - Database Migration Service
    - Homogeneous & Heterogeneous migration
    - Continuous Data Replication
    - Needs an EC2 instance for replication
    - DMS vs. Glue: DBM has continous replication, no transformation, only move data.
- AWS Step Functions
    - Design workflows
    - Error Handling & Retry mechanisms
    - History of executions
- AWS DataSync
    - Migrate from on-premises to AWS
    - DataSync agent deployed to On-premises
- MQTT (IoT messaging protocol)
    - A way to transfer lots sensor data to an ML model

## Exploratory Data Analysis

###Â General Knowledge

- Python libraries:
    - pandas
    - numpy
    - matplotlib
    - seaborn: matplotlib on steroids, pairplot, jointplot
    - scikit_learn
- Data types
    - Numerical
    - Categorical
    - Ordinal: mix of numerical & categorical. Example: star rating
- Data distribution
    - Probability density function (continous)
    - Probability mass function (discrete)
    - Normal -> continuous data
    - Poisson -> discrete data
    - Binomial: number of successes in a sequence of events.
        - Bernoulli: sequence of only one event
- Time Series Analysis
    - Trend
    - Seasonality (periodic occurrences)
    - Noise
    - Additive model (Seasonality + Trends + Noise)
        - Useful when seasonality is constant
    - Multiplicative model (Seasonality * Trends * Noise)
        - Useful when seasonality changes as the trend increases
- Hadoop
    - MapReduce
        - Software to process massive data in parallel
        - Map functions process data
        - Reduce functions combine/aggregate intermediate results into the final format
    - Spark (replaces of MapReduce)
        - In memory caching
        - Directive-acyclic graph for dependency resolution
        - Java, Scala, Python and R
        - Stream processing, ML, ...
        - Modules:
            - Spark Context (driver program)
            - Cluster Managers (Spark, YARN)
            - Executors (cache, tasks)
        - Components:
            - Spark streaming
            - Spark SQL -> similar to python pandas
            - MLLib: distributed and scalable
                - Classification
                - Regression
                - Decision trees
                - Recommendation engine
                - Clustering (K-Means)
                - LDA (topic modeling)
                - ML workflow utilities
                - SVD, PCA, statistics
            - GraphX -> data structure, not charts.
            - Zeppelin: Notebook for spark commands
    - YARN: Yet Another Resource Negotiator
    - HDFS: Hadoop Distributed File System
- Feature Engineering
    - Apply your knowledge of the data to create better features
    - Applied ML is basically feature engineering
    - Curse of Dimensionality: Too many features can be a problem
    - Selecting the relevant features to reduce dimensionality
        - Principal Component Analysis
        - K-Means
    - Missing data imputation
        - Mean replacement
            - Use median if there are outliers
            - Does not usually work well, not accurate, naive
            - Cannot be used on categorical features
        - Dropping
            - Valid approach if small number of missing values, and biased
            - Never would be the *best* approach
        - KNN: Find K "nearest" most similar rows and average their vaues. Bad with categorical
        - Deep Learning: Train a ML model to impute data, hard to achieve, but works great.
        - Regression: MICE (Multiple Imputation by Chained Equations) is state-of-art.
        - Get more data, collecting more real data.
    - Unbalanced data
        - Example: fraud detection
        - Solutions:
            - Oversampling the minority class (random or copying)
                - SMOTE (Synthetic Minority Over-sampling TEchnique): oversampmling using nearest neighbors
            - Undersampling the majority class, not recommended unless "big data" scaling issues.
            - Adjusting thresholds on probability output of the algorithm.
    - Handling outliers
        - An outlier is a point further than X stddev from the mean.
        - Remove outliers only knowing the consequences in the given context.
        - AWS Random Cut Forest algorithm to detect outlier detection
    - Binning
        - Transform numerical data into ordinal/categorical data by bucketing observations together based on ranges
        - Useful to correct errors in measurements
        - Quantile binning, categorized based on the position in the distribution
    - Transforming
        - Applying a function to a feature to make it better suited for training
    - Encoding
        - Transforming data into some new representation: OneHot encoding, Embedding
    - Scaling / Normalization
        - Make the values to be normally distributed around 0.
        - Scale features to be comparable between each other.
    - Shuffling
- TF-IDF
    - Term Frequency: how often a word occurs in a document
    - Inverse Document Frequency: how often a word occurs in an entire set of documents
    - TF / DF = TF * IDF
    - Usually log(IDF)
    - Extension: using n-grams

### AWS Related

- Amazon Athena
    - Features
        - Serverless
        - Query S3 data
        - Formats: CSV, JSON, ORC, Parquet, etc..
    - Use cases
        - Ad-hoc queries of web logs
        - Query before Redshift
        - Integration with Jupyter, Zeppelin, RStudio
        - Integrates with visual tools
        - Athena & Glue:
            - Glue Data Catalog to obtain a schema to be used by Athena
        - NOT FOR:
            - Highly formatted results -> QuickSight
            - ETL -> Glue
    - Cost model
        - 5$ TB scanned
        - Use columnar formats: ORC or Parquet
        - Glue & S3 have their own charges
    - Security
        - IAM
        - Encryption in S3
        - Cross-account using bucket policies
- Amazon QuickSight
    - Data Analysis & Visualization tool
    - Use cases
        - Focuses on business users, not developers
        - Interactive ad-hoc exploration / visualization
        - Dashboards and KPI's
        - ML Insights: Anomaly detection, Forcasting, Auto-narratives
        - NOT FOR:
            - ETL -> Glue
    - Features
        - Serverless
        - Data Sources: Redshift, RDS, Athena, EC2-hosted DBs, S3, IoT, Salesforce, etc.
        - Super-fast, Parallel, In-memory Calculation Engine (SPICE)
        - Columnar storage
        - Limited to 10GB per user
        - Q: answer questions with NLP (training required: dates, topics & datasets)
        - Paginated Reports designed to be printed
        - Dashboards
            - Read only when shared
            - Visual Types: AutoGraph, Bar Chart/Histogram, Line graphs, Scatter plots, Heat Maps, Pie graphs, Tree Maps, Pivot tables, KPIs, Geo Charts, Donuts, Gauge, Word cloud.
    - Security
        - MFA
        - VPC connectivity
        - Row-level & column-level security
        - Private VPC Access
        - User Management: IAM, SAML, Active Directory or email signup
- Elastic MapReduce (EMR) & Hadoop
    - Managed Hadoop framework on EC2 instances
        - Spark, HBase, Presto, Flink, Hive & more
        - EMR Notebooks
    - Use cases
        - Distribute the load to preprocess massive datasets
    - Features
        - Cluster
            - Collection of EC2 instances -> each instance is a Node
            - Node types:
                - Master: m4.large if nodes < 50 else m4.xlarge
                - Core:
                    - m4.large
                    - t2.medium if a lot of i/o
                    - m4.xlarge for improved performance
                    - <see slides>
                 Task: spot instances
            - Core nodes interact with HDFS
            - Task nodes are optional and only execute tasks but do not store data
        - Usage: Transient & Long-Running clusters
        - AWS integration: EC2, VPC, S3, CloudWatch, IAM, CloudTrail, DataPipelines
        - Storage
            - HDFS, distributed, scalable file system -> Ephemeral
            - EMRFS: use S3 as if it were HDFS
            - Local file system
            - EFS for HDFS
        - EMR Notebook:
            - Studios
            - Similar to Zeppelin, but additional AWS features
            - Only available from AWS Console
            - Backed in S3
            - Free to use
    - Cost model
        - Pay per hour
        - Provision new nodes upon failure
        - Add and remove tasks nodes on the fly
        - Resize core nodes in a running cluster
    - Security
        - IAM policies & roles
        - Kerberos
        - SSH
        - Lake Formation config.
        - Apache Ranger for Hadoop / Hive
- SageMaker Ground Truth
    - Having humans to tag data
    - Useful for imputing missing data
    - Big data set
    - It learns from the tagging and only asks for clarification ambiguous cases.
    - Human labelers
        - Mechanical Turk
        - Internal team
        - Profesisonal labeling companies
    - Pre-trained models
        - Rekognition for image recognition
        - Comprehend for text analysis and topic modelling
    Ground Truth *Plus*
        - Hire "AWS Experts" to handle the project for you.
        - Track progress in Plus Project Portal
        - Get labeled data from S3.

## Modelling

### General

#### Neural Networks
- DeepLearning: a neural network with more than one layer.
- Types of neural network:
    - Feedforward -> common classification or regression
    - Convolutional (CNN) -> image classification
    - Recurrent (RNN) -> sequences (time, words, etc.)
- Activation functions
    - Linear activation
    - Binary step function
    - Non-linear functions (backpropagation, multiple layers)
        - Sigmoid / Logistic / TanH
        - Rectified Linear Unit (ReLU)
        - Leaky/Parametric ReLU (small slope for negative numbers)
        - Other ReLU (swish, maxout)
    - Softmax: usually last layer of a classification NN.
    - How to choose one?
        - Multi classification -> softmax on output layer
        - RNN -> TanH
        - Everything else -> ReLU > Leaky ReLU, PReLU, Maxout > Swish (deep networks)
- Convolutional Neural Networks
    - Find patterns in the data
    - Use cases: images, translation, sentence classification, sentiment analysis
    - Keras:
        - Input Data: width * height * color channels
        - Conv2D layer
        - MaxPooling2D
        - Flatten
        - Dropout
        - Dense
        - Dropout
        - Softmax
    - Architectures: (specific arrange of layers)
        - LeNet-5: handwriting recognition
        - AlexNet: image classification
        - GoogLeNet: even deeper
        - ResNet (Residual Network): deeper
- Recurrent Neural Networks
    - Sequence of data: time-series, events, text, music, etc.
    - Past behavior of the model is fed into the current prediction
    - Older data might have lower relevance
    - Topologies
        - Sequence to sequence: predict stock prices
        - Sequence to vector: words in a sentence to sentiment
        - Vector to sequence: captions from an image
        - Encoder -> Decoder: sequence -> vector -> sequence
    - Training usually requires stopping the backpropagation
    - Types of cells:
        - LSTM: long short-term memory
        - GRU: Gated Recurrent Unit, simplification of LSTM
#### Transformers & Modern NLP
- Transformer deep learning architecture
    - self-attention allow processing words in parallel
    - BERT: Bi-directional Encoder Representations from Transformers
    - GPT: Generative Pre-trained Transformer
- Transfer Learning
    - Use pre-trained models
    - Hugging Face has a set of pre-trained models
    - Integrates iwth Sagemaker
    - Approaches
        - Use it as-is
        - Fine-tune
            - Format input data as it was when the model was trained originally
            - Start training the model with low learning rate.
        - Add new trainable layers on top of the existing model
        - Retrain from scratch using the architecture
            - beware of data and money!

### AWS Specific

#### Where to run and train deep models
- EMR supports Apache MXNet and GPU
- Appropriate EC2 types for deep learning: P3, P2, G3, G5g, P4d - A100 "UltraClusters"
- Generative AI: Trn1, Trn1n instances (high bandwidth between nodes),
- Inference: Inf2


## ML Operations

## Exam Questions

## Generative AI
